# -*- coding: utf-8 -*-
"""MLINTERN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/139XrkrVWCjg_TLdLhYdz0589odVEZXnW

## Mean Mode Median SD Varience
"""

#mean mode median sd var
import numpy as np
data=[32,30,24,20,5,30,60,10,75]
mean=np.mean(data)
print("mean of the data: ",mean)
median=np.median(data)
print("median of the data: ",median)
mode=np.bincount(data).argmax()
print("mode of the data: ",mode)
sd=np.std(data)
print("standard deviation of the data: ",sd)
var=np.var(data)
print("variance of the data: ",var)

"""## Mean Mode Median SD Varience with pandas"""

import pandas as pd
data = pd.Series([32, 30, 24, 20, 5, 30, 60, 10, 75])
mean = data.mean()
print("mean of the data: ", mean)
median = data.median()
print("median of the data: ", median)
mode = data.mode()[0]
print("mode of the data: ", mode)
sd = data.std()
print("standard deviation of the data: ", sd)
var = data.var()
print("variance of the data: ", var)

"""## Mean Mode Median SD Varience with scipy"""

import numpy as np
from scipy import stats
data = np.array([32, 30, 24, 20, 5, 30, 60, 10, 75])
mean = np.mean(data)
print("mean of the data: ", mean)
median = np.median(data)
print("median of the data: ", median)
mode = stats.mode(data).mode
print("mode of the data: ", mode)
sd = np.std(data)
print("standard deviation of the data: ", sd)
var = np.var(data)
print("variance of the data: ", var)

"""## Mean Mode Median SD Varience with statistics"""

import statistics
data = [32, 30, 24, 20, 5, 30, 60, 10, 75]
mean = statistics.mean(data)
print("mean of the data: ", mean)
median = statistics.median(data)
print("median of the data: ", median)
mode = statistics.mode(data)
print("mode of the data: ", mode)
sd = statistics.stdev(data)
print("standard deviation of the data: ", sd)
var = statistics.variance(data)
print("variance of the data: ", var)

"""## Creating a file from the data set"""

#creating a dataset
import pandas as pd
data={
    'name': ['suny','huny','buny','jony','rony'],
    'age': [32,34,45,64,40],
    'height': [175,160,180,162,179]
}

df=pd.DataFrame(data)
df.to_csv('data.csv',index=False)
df

"""## Data pre processing"""

#import necessary libraries
import numpy as np
import pandas as pd
#read the file
df=pd.read_csv('HumanBodyPerformance.csv')
df

df.shape

df.info()

df.describe()

df['age'].mean()

df.head(10)

df.tail(10)

"""### Q:Write the code to read the middel of the dataset?"""

df.iloc[len(df) // 2 - 5 : len(df) // 2 + 5]

"""## names of columns"""

df.columns

df['sit-ups counts'].mean()

df['sit-ups counts'].mode()

df.dtypes

"""## missing values"""

df.isnull()

df.isnull().sum()

df.isnull().sum().sum()

"""## Get the count of unique values in a specific columns"""

df.columns

df['age'].value_counts()

df['gender'].value_counts()

#number of unique values in each column
df.nunique()

"""## Checkin the duplicate value in the column and remove it"""

df.duplicated()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.duplicated().sum()

df.shape

"""## How many numerical and categorical data are there"""

numerical=df.select_dtypes(exclude=object)
numerical

categorical=df.select_dtypes(include=object)
categorical

"""# Data Normalization

## Z Score Data Normalization
"""

import pandas as pd
import numpy as np

data=[200,300,400,600,1000]

standard_deviation = np.std(data)
print(standard_deviation)

mean= np.mean(data)
print(mean)

V=0

for i in range(0,5):
    V=(data[i]-mean)/standard_deviation
    print(V)

#other method for z score normalization
import pandas as pd
from sklearn.preprocessing import StandardScaler
#creating the dataset
data={
    'bedroom':[1,2,3,4,5],
    'bathroom':[1,2,2,3,2],
    'sqrt_living':[500,1000,1500,2000,2500],
    'age_of_the_house':[5,10,15,20,25],
    'price':[100000,150000,200000,250000,300000]
}
d=pd.DataFrame(data)
d

scaler = StandardScaler()
standardized_data = scaler.fit_transform(d)
standardized_data

normalized_data = pd.DataFrame(standardized_data, columns=d.columns)
normalized_data

"""## Min Max Data Normalization"""

import pandas as pd
import numpy as np

data=[200,300,400,600,1000]

mina=min(data)
maxa = max(data)

V=0

for i in range(0,5):
    V=(data[i]-mina)/(maxa-mina)
    print(V)

#other method for min max normalization
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
#creating the dataset
data={
    'bedroom':[1,2,3,4,5],
    'bathroom':[1,2,2,3,2],
    'sqrt_living':[500,1000,1500,2000,2500],
    'age_of_the_house':[5,10,15,20,25],
    'price':[100000,150000,200000,250000,300000]
}
d=pd.DataFrame(data)
d

scalar=MinMaxScaler()
scalar.fit_transform(d)

normalized_data=pd.DataFrame(scalar.fit_transform(d),columns=d.columns)
normalized_data

"""## Decimal Scaler Normalization"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import FunctionTransformer
#creating the dataset
data={
    'bedroom':[1,2,3,4,5],
    'bathroom':[1,2,2,3,2],
    'sqrt_living':[500,1000,1500,2000,2500],
    'age_of_the_house':[5,10,15,20,25],
    'price':[100000,150000,200000,250000,300000]
}
d=pd.DataFrame(data)
d

#create the Decimal scaler Library by our own
def decimal_scaler(x):
    max_abv=np.max(np.abs(x))
    scale=10**np.ceil(np.log10(max_abv))
    return x/scale

transformer=FunctionTransformer(decimal_scaler)
decimal_scaler=transformer.fit_transform(d)
decimal_scaler

